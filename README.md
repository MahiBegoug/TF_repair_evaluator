# TFRepair Evaluation

This script evaluates the repairs generated by LLMs by applying them to the original Terraform files and running `terraform validate`. Note that this only checks for syntactic validity (plausible fixes), not functional correctness.

## Usage

To run the evaluation, use the `main.py` script.

### Arguments

- `--fixes-csv`: **(Required)** Path to the CSV file containing the LLM fixes.
- `--output-csv`: **(Optional)** Path to the output CSV file where diagnostics will be saved. Default is `repair_results.csv`.
- `--clones-dir`: **(Optional)** Path to the directory containing the cloned repositories. Default is `../TFReproducer/clones`.

### Example Command

```bash
python main.py --fixes-csv llms_fixes_results/fullfile_llm_repairs.csv --output-csv repair_eval_diagnostics.csv --clones-dir ../TFReproducer/clones
```

## Output

The script produces a CSV file (specified by `--output-csv`) containing any diagnostics found after applying the fixes. If the repair is successful, the file will contain headers but no diagnostic rows for that file.

## Repair Pipeline Workflow

We provide an automated pipeline to batch process raw LLM responses, apply fixes, and validate them.

### 1. Configuration

The pipeline is controlled by `repair_config.json` in the project root.

```json
{
    "input_dir": "llm_responses",
    "output_dir": "llms_fixes_results",
    "clones_dir": "../TFReproducer/clones",
    "models": ["gemini"]
}
```

-   `input_dir`: Directory containing raw LLM response CSVs.
-   `output_dir`: Directory where validated results (with `plausible_fix` column) will be saved.
-   `clones_dir`: Path to the directory containing the cloned repositories.
-   `models`: List of model names to process (e.g., `["gemini"]`). Leave empty `[]` to process all files.

### 2. Running the Pipeline

To run the pipeline based on the configuration:

```bash
python run_repair_pipeline.py
```

This will:
1.  Read raw CSVs from `input_dir`.
2.  Filter files based on the `models` list.
3.  Apply fixes and run `terraform validate`.
4.  Save the results (outcomes and diagnostics) to `output_dir`.

## Evaluation Workflow

We provide an automated workflow to evaluate LLM repairs using the `pass@k` metric.

### 1. Configuration
 
The evaluation is controlled by `evaluation/evaluation_config.json`. You can modify this file to specify:
- Input directory for LLM fixes (`fixes_dir`).
- Input directory for LLM fixes (`fixes_dir`).
- Output directory for repair outcomes (`output_dir`).
- Output directory for pass@k analysis (`results_dir`).
- Models to evaluate (`models`).
- Whether to generate synthetic data (`generate_synthetic_data`).
- Data type filter (`data_type`): Controls which files to process based on their filename.
    - `"synthetic"`: Only process files containing "synthetic" in the name.
    - `"real"`: Only process files *without* "synthetic" in the name.
    - `"all"`: Process all CSV files.

Example `evaluation/evaluation_config.json`:
```json
{
    "fixes_dir": "llms_fixes_results",
    "fixes_dir": "llms_fixes_results",
    "output_dir": "evaluation/data",
    "results_dir": "evaluation/results",
    "clones_dir": "../TFReproducer/clones",
    "k_values": [1, 5, 10],
    "models": [],
    "generate_synthetic_data": false,
    "data_type": "all"
}
```

### 2. Running Evaluation

To evaluate all models based on the configuration, run:

```bash
python evaluation/evaluate_all_models.py
```

This script will:
1.  **Generate Data** (Optional): If `generate_synthetic_data` is true, it generates synthetic fix files.
2.  **Apply & Validate**: For each model's fix file, it applies the fixes and runs `terraform validate`.
3.  **Calculate Metrics**: It computes `pass@k` scores for each model.
4.  **Save Results**:
    - Individual results: `evaluation/results/<model>_pass_at_k.csv`
    - Aggregated summary: `evaluation/results/summary_pass_at_k.csv`

For more details on the metric and synthetic data generation, see [evaluation/README.md](evaluation/README.md).

